---
layout: modular
title: SwiftLlamaEmbed
lang: en

# Module Configuration - All settings are defined in Markdown
modules:
  # Hero Module - Required
  hero:
    enabled: true
    title: "SwiftLlamaEmbed"
    subtitle: "Swift • AI • Embeddings"
    description: "A powerful Swift wrapper for llama.cpp that brings text embedding capabilities to iOS, macOS, tvOS, and visionOS applications."
    cta_text: "Get Started"
    cta_link: "#project"
    hero_image: "/assets/images/hero-avatar.jpg"

  # Project Module
  project:
    enabled: true
    title: "Project Overview"
    subtitle: "High-performance text embeddings for Swift applications"
    description: "SwiftLlamaEmbed provides a clean, Swift-native interface to llama.cpp's embedding capabilities. Built with performance and ease-of-use in mind, it enables developers to integrate powerful text embedding functionality into their apps with just a few lines of code."
    promo_video: "/assets/videos/codeflow-promo.mp4"
    main_image: "/assets/images/project/main-preview.jpg"
    demo_url: "https://github.com/gradinnovate/SwiftLlamaEmbed"
    github_url: "https://github.com/your-gradinnovate/SwiftLlamaEmbed"
    download_url: "https://github.com/gradinnovate/SwiftLlamaEmbed"
    technologies:
      - "Swift"
      - "llama.cpp"
      - "XCFramework"
      - "GGUF"
      - "Metal"
      - "Core ML"
    

  # About Module
  about:
    enabled: true
    title: "About SwiftLlamaEmbed"
    description: "SwiftLlamaEmbed bridges the gap between powerful AI embedding models and Swift applications. It provides a native Swift interface to llama.cpp, enabling developers to run embedding models locally on Apple devices with optimal performance and privacy."
    about_image: "/assets/images/about-image.jpg"
   
    skills:
      - name: "Multi-Platform Support"
        icon: "fas fa-mobile-alt"
        description: "Native support for iOS and macOS"
      - name: "High Performance"
        icon: "fas fa-rocket"
        description: "Optimized with Metal and CPU acceleration"
      - name: "Privacy First"
        icon: "fas fa-shield-alt"
        description: "Local processing with no data sent to external servers"

  # Features Module
  features:
    enabled: true
    title: "Why Choose SwiftLlamaEmbed"
    subtitle: "Built for modern Swift applications with performance in mind"
    features_list:
      - title: "Easy Integration"
        icon: "fas fa-code"
        description: "Simple Swift API with comprehensive documentation"
      - title: "Local Processing"
        icon: "fas fa-microchip"
        description: "Run embedding models entirely on-device for privacy"
      - title: "Universal Compatibility"
        icon: "fas fa-cubes"
        description: "Works with any GGUF format embedding model"

  # Video Demo Module
  video_demo:
    enabled: false
  

  # Testimonials Module
  testimonials:
    enabled: false
    title: "What Developers Say"
    testimonials_list:
      - content: "SwiftLlamaEmbed made it incredibly easy to add semantic search to our iOS app. The API is clean and the performance is outstanding."
        author: "Sarah Johnson"
        position: "iOS Developer"
        company: "TechCorp Inc."
        avatar: "/assets/images/testimonials/sarah.jpg"
      - content: "Finally, a native Swift solution for embeddings! The local processing ensures our users' privacy while delivering great performance."
        author: "Jane Chen"
        position: "Lead Developer"
        company: "PrivacyFirst Apps"
        avatar: "/assets/images/testimonials/jane.jpeg"

  # Contact Module
  contact:
    enabled: true
    title: "Get Started Today"
    subtitle: "Ready to add powerful text embeddings to your Swift app? Check out our documentation and examples."
    contact_form_enabled: true
    show_social: true

# SEO Settings
description: "SwiftLlamaEmbed - A powerful Swift wrapper for llama.cpp that brings text embedding capabilities to Apple platforms"
keywords: "swift, llama.cpp, embeddings, ios, macos, ai, machine learning, text processing"
---

<!-- All content is dynamically generated by modules based on the above configuration -->
